# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""" Query Engine Service """

import functools
import gc
import json
import tempfile
import time
from typing import List, Optional, Generator, Tuple, Dict
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from tqdm.auto import tqdm
from pathlib import Path
from common.utils.logging_handler import Logger
from common.models import UserQuery, QueryResult, QueryEngine
from common.utils.errors import ResourceNotFoundException
from common.utils.http_exceptions import InternalServerError
from common.utils.logging_handler import Logger
from google.cloud import aiplatform
from google.cloud import storage
from vertexai.preview.language_models import TextEmbeddingModel
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import CSVLoader, PDFMinerLoader

from config import PROJECT_ID

# text chunk size for embedding data
CHUNK_SIZE = 1000

# Create a rate limit of 300 requests per minute. Adjust this depending on your quota.
API_CALLS_PER_SECOND = 300 / 60

# According to the docs, each request can process 5 instances per request
ITEMS_PER_REQUEST = 5

# embedding dimensions generated by TextEmbeddingModel
DIMENSIONS = 768

async def query_generate(prompt: str, query_engine: str,
                         user_query: Optional[UserQuery] = None) -> QueryResult:
  pass
  

def query_engine_build(doc_url: str, query_engine: str, params: Dict) -> QueryEngine:
  # build document index
  build_doc_index(doc_url, query_engine)
    
  # create model
  is_public = params.get("is_public", True)
  user_id = params.get("user_id")
  query_engine = QueryEngine(created_by=user_id, is_public=is_public)
  query_engine.save()

  return query_engine


def build_doc_index(doc_url:str, query_engine: str) -> bool:
  """
  Build the document index.  
  Supports only GCS URLs initially, containing PDF and CSV files.
  """
  # download files to local directory
  doc_filepaths = _download_files_to_local(doc_url)

  # ME index name and description
  index_name = query_engine + "_MEindex"
  index_description = \
    "Matching Engine index for LLM Service query engine: " + query_engine

  # bucket for ME index data
  bucket_name = f"{index_name}_ME_data"
  bucket = storage_client.create_bucket(bucket_name)
  bucket_uri = f"gs://{bucket.name}"

  # use langchain text splitter
  text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)

  # add embeddings for each doc to index data stored in bucket
  for doc in doc_filepaths:
    doc_name, doc_filepath = doc

    # read doc data and split into text chunks
    doc_text_list = _read_doc(doc_name, doc_filepath)  
    text_chunks = []
    for text in doc_text_list:
      text_chunks.extend(text_splitter.split_text(text))
    
    # generate embedding data and store in local dir
    embeddings_file_path = _generate_index_data(doc_name, text_chunks)

    # copy data files up to bucket
    remote_folder = f"{bucket_uri}/{doc_name}/"
    blob = bucket.blob(remote_folder)
    data_path = f"{embeddings_file_path}/*" 
    blob.upload_from_filename(data_path)

  # create ME index
  tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(
      display_name=index_name,
      contents_delta_uri=bucket_uri,
      dimensions=DIMENSIONS,
      approximate_neighbors_count=150,
      distance_measure_type="DOT_PRODUCT_DISTANCE",
      leaf_node_embedding_count=500,
      leaf_nodes_to_search_percent=80,
      description=index_description,
  )
    

def _download_files_to_local(doc_url: str) -> List[Tuple[str, List[str]]]:
  client = storage.Client(project=PROJECT_ID)
  docs = []
  for blob in client.list_blobs(doc_url):
    # skip directories for now
    if blob.name.endswith("/"):
      continue
    # Create a blob object from the filepath
    with tempfile.TemporaryDirectory() as temp_dir:
      file_path = f"{temp_dir}/{self.blob}"
      os.makedirs(os.path.dirname(file_path), exist_ok=True)
      # Download the file to a destination
      blob.download_to_filename(file_path)
      docs.append((blob.name, file_path))
  return docs


def _read_doc(doc_name:str, doc_filepath: str) -> List[str]:
  """ Read document and return content as a list of strings """
  doc_extension = doc_name.split(".")[-1]
  doc_extension = doc_extension.lower()
  doc_text_list = None
  loader = None
  if doc_extension == "txt":
    with open(doc_filepath) as f:
      doc_text = f.read()
    doc_text_list = [doc_text]
  elif doc_extension == "csv":
    loader = CSVLoader(file_path=doc_filepath)
  elif doc_extension == "pdf":
    loader = PDFMinerLoader(file_path=doc_filepath)
  else:
    # return None if doc type not supported
    pass 

  if loader is not None:
    langchain_document = loader.load()
    doc_text_list = [section.content for section in langchain_document]

  return doc_text_list

def _encode_texts_to_embeddings(
    sentence_list: List[str]) -> List[Optional[List[float]]]:
  model = TextEmbeddingModel.from_pretrained("textembedding-gecko@001")
  try:
    embeddings = model.get_embeddings(sentence_list)
    return [embedding.values for embedding in embeddings]
  except Exception:
    return [None for _ in range(len(sentence_list))]


# Generator function to yield batches of sentences
def _generate_batches(
    sentences: List[str], batch_size: int
) -> Generator[List[str], None, None]:
    for i in range(0, len(sentences), batch_size):
        yield sentences[i : i + batch_size]


def _get_embedding_batched(
    sentences: List[str], api_calls_per_second: int = 10, batch_size: int = 5
) -> Tuple[List[bool], np.ndarray]:

  embeddings_list: List[List[float]] = []

  # Prepare the batches using a generator
  batches = _generate_batches(sentences, batch_size)

  seconds_per_job = 1 / api_calls_per_second

  with ThreadPoolExecutor() as executor:
    futures = []
    for batch in tqdm(
        batches, total=math.ceil(len(sentences) / batch_size), position=0
    ):
      futures.append(
          executor.submit(functools.partial(_encode_texts_to_embeddings), batch)
      )
      time.sleep(seconds_per_job)

    for future in futures:
      embeddings_list.extend(future.result())

  is_successful = [
    embedding is not None for sentence, embedding in zip(sentences, embeddings_list)
  ]
  embeddings_list_successful = np.squeeze(
    np.stack([embedding for embedding in embeddings_list if embedding is not None])
  )
  return is_successful, embeddings_list_successful


def _generate_index_data(doc_name: str, text_chunks: List[str]) -> str

  for i in range(len(text_chunks)):

    # Create temporary file to write embeddings to
    embeddings_file_path = Path(tempfile.mkdtemp())
  
    # Create a unique output file for each set of embeddings
    chunk_path = embeddings_file_path.joinpath(
        f"{doc_name}_{i}.json"
    )

    with open(chunk_path, "a") as f:

      # Convert batch to embeddings
      is_successful, question_chunk_embeddings = _get_embedding_batched(
          sentences=text_chunks,
          api_calls_per_second=API_CALLS_PER_SECOND,
          batch_size=ITEMS_PER_REQUEST,
      )
  
      # Append to file
      embeddings_formatted = [
        json.dumps(
          {
            "id": str(i),
            "embedding": [str(value) for value in embedding],
          }
        )
        + "\n"
        for embedding in question_chunk_embeddings
      ]
      f.writelines(embeddings_formatted)

  # clean up any large data structures
  gc.collect()

  return embeddings_file_path
